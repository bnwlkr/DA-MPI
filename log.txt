
Date:   Tue Jul 2 14:09:11 2019 -0700

    Everything is working now


Date:   Tue Jul 2 11:32:50 2019 -0700

    it seems that DAMPI wins only in the extreme cases. It turns out that the synchronization that the current implementation requires in the airlock is where the bulk of the time is going


Date:   Tue Jul 2 10:57:36 2019 -0700

    DAMPI finally beat a normal implementation. I'm going to keep looking for possible speedups.


Date:   Mon Jul 1 23:33:30 2019 -0700

    everything is working on cyclops. All I need to do now is configure latencies for the demo.


Date:   Mon Jul 1 16:55:56 2019 -0700

    introducing artificial latency seems to be working well enough. just have to test the whole system on cyclops next, and see if anything actually works as it should


Date:   Mon Jul 1 15:09:07 2019 -0700

    finished separating the profiler into its own executable.


Date:   Sun Jun 30 11:37:35 2019 -0700

    update gitignore


Date:   Sun Jun 30 11:36:45 2019 -0700

    things are working fine. All that's left is to optimize as much as possible so that the DAMPI implementation adds as little overhead as possible. I should aim to do this optimizaiton on the cyclops cluster where I will be deoming from. Since the DAMPI library adds some factorial operations, I'll need to see how much overhead they in fact created in a situation with many nodes (probably way too much). Some ways I have thought of for speeding things up
    : move profiling to separate executable (output a file that can be read by a DAMPI program every run), develop some sort of heuristic to determine if a swap should occur, and check for swaps much less frequently.


Date:   Sat Jun 29 23:26:32 2019 -0700

    Things seem to be working fine. Just need to workout the 'one-last-thing' issue that is complicated by the migration. Then I'll try to get everything spinning on cyclops


Date:   Sat Jun 29 12:36:11 2019 -0700

    Sieve implementation is going alright. Just have two segfault cases to deal with, otherwise everything is working.


Date:   Fri Jun 28 23:11:48 2019 -0700

    getting close to a working prime implementation. The DAMPI library necessitates a little bit more focus on synchronization.


Date:   Thu Jun 27 22:10:02 2019 -0700

    Simplified the comms logic based on the aforementioned assumption. Things seem to be working. Tomorrow I will attempt to implement the sieve, and then its just a matter of getting it all running on cyclops, developing a few other interesting communication patterns, artifically introducing latency, and prepping for demo


Date:   Thu Jun 27 21:12:14 2019 -0700

    I think I just realized that all of my efforts to fix the issue of a blocking send causing deadlock issues with the blocking airlock were in vain. Since the programmer knows that the sends and receives are blocking, they should be paired off in the body of the loop between processes. Oh well. I'm going to pare down the code given this assumption. This commit is here in case the assumption doesn't hold in some important cases.


Date:   Thu Jun 27 19:19:09 2019 -0700

    First draft implementations of pre-emptable send and recv done. So far it seems to be working fine. Need to do more in-depth tracing of the program to make sure evrything is working as it should. Stress tests needed as well.


Date:   Thu Jun 27 12:24:09 2019 -0700

    processes waiting on eachother's requests doesn't work either. Any sort of blocking waiting apart from the one in the airlock creates a deadlock scenario, and the looping test strategy means that another migration could occur while the processes are waiting for eachother's requests, so they would need to keep waiting on the other side of the migration, and on the other side of the next migration, etc. My first idea to solve this was a timeout in sends and receives that send the cancel the request and send the process back to the airlock. While it might work, this is meant to be a performance-enhancing library. My current idea (the one that I haven't thought of any issues with yet) is to have every process broadcast the fact that its in the migration zone of the airlock, so that any pending sends and recvs with that process as the target can be cancelled, and resumed later.


Date:   Thu Jun 27 10:42:13 2019 -0700

    sending request objects between nodes is a no-go. The MPI_Request type is actually just a pointer to a big scary struct called ompi_request_t that contains more pointers and apparently some polymorphic stuff (super pointer o.O). A workaround for this might be for the swapped nodes to wait on eachother's requests and then notify the other on completion - thats what I'm going to try next anyway


Date:   Thu Jun 27 09:47:30 2019 -0700

    started on the implementation of a coroutine (ish) approaching using send/recv line numbers to jump around migrated functions


Date:   Wed Jun 26 18:42:41 2019 -0700

    Apart from sending messages to the wrong ranks due to migration, there is the issue of other ranks waiting in the airlock while another rank is on a blocking send. I can make MPI_Send blocking by having its internals busy wait on an asynchronous send, repeatedly joining the airlock in between receipt tests. The way that process migration works currently is that the new function to run is called from the airlock, so a process could be swapped during a send operation. This means that it might not get to the 'send completed state' before migrating to a different function. A not-so-elegant approach would be to have the processes jump to the appropriate send/receive call when they enter the new function, and start waiting on the completion condition that the other process was waiting on. This would involve sending request objects between migrating nodes (along with suitcases and everything else). Elegance became a secondary concern a while ago anyway.


Date:   Mon Jun 24 23:42:11 2019 -0700

    migration is working fine between processes on the mac, but when I introduce the remote raspberry pi I'm getting deadlock... so weird. I think it has to do with non-exclusive swap requests because of 'rdma' issues between the pi and the mac. will investigate tomorrow


Date:   Mon Jun 24 13:20:39 2019 -0700

    simplified the suitcase exchange to use a single MPI_Sendrecv (exchanges and synchronizes). Also verified that the suitcase exchange actually works. Got to work on the function switching logic now


Date:   Sun Jun 23 13:23:53 2019 -0700

    I think that I am going to switch simple frequency counting to an exponential or weighred moving average in order to make the migration system more in touch with a more recent state of the communication system so that it can adapt it accordingly


Date:   Sun Jun 23 12:14:19 2019 -0700

    decided to just use the maximum suitcase size as the suitcase size for all ranks. This simplifies a lot of memory allocation stuff during execution. Migration seemsto be working correctly. More testing to come


Date:   Sat Jun 22 21:57:36 2019 -0700

    The airlock seems to have its basic functionality. Next thing to do is verify that the suitcases are being swapped correctly, and then move onto actually switching subroutines (coroutines?) I might use setcontext for that, because I can link the routines back to main - otherwise i can do some smart fall-through back to main.


Date:   Fri Jun 21 20:11:23 2019 -0700

    more work on the airlock - its turning out to be pretty hard. Evenutally, I will want to have the minimal amount of synchronization that still works in the airlock. For now, though, I think i'll just try to get things working with any level of synchronization (less regard for performance). need to consider what happens when a migration is requested after some nodes have dropped out of the cluster...


Date:   Fri Jun 21 12:53:36 2019 -0700

    started on the airlock logic. also had to consider what would happen on a blocking send to a node that just migrated. to fix this problem, I will need to have a special receive that will manage message forwarding


Date:   Thu Jun 20 12:00:26 2019 -0700

    changed the delay data sync so that all processes write to a single node and then read from that node. should be O(n) instead of O(n^2).


Date:   Wed Jun 19 21:59:39 2019 -0700

    I've decided to go the 'suitcase' route. Every rank will have a function that it runs, and every rannk will have a suitcase that it uses to hold its state. The suitcases will be passed around in migrations. Every proc will have info about the sizes of all of the rank's suitcases so that they can easily shift them around.


Date:   Wed Jun 19 20:30:49 2019 -0700

    testing whether or not a node should migrate is working fine. Currently, the threshold that determines whether the difference between the configuration values is big enough to warrant a migration is static. Since the configuration value is a function of the number of messages sent, I think that the threshold value should rise as the total number of messages sent rises -- otherwise swaps will arbitrarily become more common once more messages have been sent


Date:   Wed Jun 19 17:36:59 2019 -0700

    started on computing the values of rank-process configurations by considering delays and communication frequency. still need to work on how the actual migration will happen. there is a tradeoff between flexibility ad ease-of-use for the library user. I think my preference is to have the system be able to run heterogeneously (on different architectures), so I will sacrifice a bit of usability - such that the library isn't really something you could use on exisitng MPI code but would have to be used from the start


Date:   Tue Jun 18 14:00:07 2019 -0700

    I think the use of ucontext.h is promising. It has some user-defined stack mechanism, so I might be able to just copy stacks between processes and use makecontext to jump between functions. More research needed


Date:   Mon Jun 17 20:21:45 2019 -0700

    tried doing the process swapping using setjmp but it didn't work. I think that setjmp will only operate within the same thread, process, and architecture. Its looking like I will have to implement my own version for this particular use case. Going to also try the deprecated ucontext to see if that one works


Date:   Sun Jun 16 21:54:49 2019 -0700

    frequency tables hosted on the best node are working. starting on send logic.


Date:   Sun Jun 16 16:19:10 2019 -0700

    going back in time because it seems like the windows have stopped working..


Date:   Sun Jun 16 13:38:22 2019 -0700

    started on window creation for the comms frequency table. it seems that local accesses aren't updating the window memory for MPI_Get operations. working on that next


Date:   Fri Jun 14 22:48:42 2019 -0700

    profiling edge delays between heterogeneous MPI nodes seems to be working :)


Date:   Fri Jun 14 10:23:24 2019 -0700

    in order to properly measure the link speeds in the system, I need to make the measurement occur symmetrically across ranks (i.e. no rank should behave differently as in the origin gather method).


Date:   Thu Jun 13 23:04:11 2019 -0700

    managed to install openmpi from the source with heterogeneous mode enabled, and got a process running on the raspberry pi and on the mac. next thing to do is designed the new profiling system that uses PMPI on every call


Date:   Thu Jun 13 15:19:07 2019 -0700

    for some reason all communications with node 0 are much faster than those between other nodes. It seems to me that node 0 is behaving similarly to the rest of the nodes (except for the minor difference that it can also receive results. I think that a cleaner solution to this problem is to use PMPI to measuer frequency and latencies simultaneously, and for nodes to use windows to share the information among themselves.


Author: Ben Walker <bnwlkr@Bens-MacBook-Pro.local>
Date:   Mon Jun 10 23:53:51 2019 -0700

    Basic time measurment functionality seems to be working. I'm sure i am making some bad assumptions about the way that MPI message passing works. Will do more research

Author: Ben Walker <bnwlkr@mail.com>
Date:   Fri Jun 7 17:06:29 2019 -0700

    Initial commit
